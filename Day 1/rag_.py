# -*- coding: utf-8 -*-
"""RAG .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hH76avzAKgNR07N2C2oyWKNvOHKbPhio
"""

# ⚙️ Step 1: Install necessary libraries
!pip install -q PyMuPDF faiss-cpu sentence-transformers

# 📚 Step 2: Import libraries
import fitz  # For PDF reading
import faiss
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from IPython.display import display
from google.colab import files

# 📤 Step 3: Upload a PDF
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]  # Automatically picks the uploaded file

# 📄 Step 4: Extract text from PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    return "\n".join([page.get_text() for page in doc])

full_text = extract_text_from_pdf(pdf_path)

# ✂️ Step 5: Chunk the text
def chunk_text(text, chunk_size=1000, overlap=100):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i+chunk_size])
        chunks.append(chunk)
    return chunks

chunks = chunk_text(full_text)

# 🧠 Step 6: Embed chunks using SentenceTransformer
model = SentenceTransformer("all-MiniLM-L6-v2")
chunk_embeddings = model.encode(chunks)

# 🔍 Step 7: Build FAISS index
dimension = chunk_embeddings[0].shape[0]
index = faiss.IndexFlatL2(dimension)
index.add(chunk_embeddings)

# ❓ Step 8: Get question from user and retrieve top chunks
def retrieve_context(question, k=1):
    q_embedding = model.encode([question])
    _, top_k_indices = index.search(q_embedding, k)
    return [chunks[i] for i in top_k_indices[0]]

# 🤖 Step 9: Generate answer using simple summarization
def generate_answer(question, context_chunks):
    context = "\n\n".join(context_chunks)
    print("🔍 Question:", question)
    print("\n📘 Context:\n", context)
    print("\n🧠 Answer (based on context):")
    # Just a basic summarization for now
    print("->", "The answer is likely discussed in the context above. Read the context to extract it manually or enhance this pipeline with a local LLM.")

# 🧪 Step 10: Try it out
question = input("Enter your question: ")
top_chunks = retrieve_context(question)
generate_answer(question, top_chunks)